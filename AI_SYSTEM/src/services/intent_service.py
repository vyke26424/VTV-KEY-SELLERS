import google.generativeai as genai
from src.config import settings
import json
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from typing import Dict, Any, Optional
from src.services.embedding_cache import embedding_cache
import numpy as np
import os
import hashlib
from sentence_transformers import SentenceTransformer
import redis
from redis.commands.search.field import TextField, VectorField

from redis.commands.search.index_definition import IndexDefinition, IndexType

from redis.commands.search.query import Query


REDIS_HOST = "localhost"
REDIS_PORT = int(6379)

EMBEDDING_MODEL_NAME = 'AITeamVN/Vietnamese_Embedding'
VECTOR_DIM = 1024
INDEX_NAME = 'intent_cache_1024_idx'

class SemanticCache :
    def __init__(self):
        try : 
            self.r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=False)
            self.r.ping()
            print('Káº¿t ná»‘i Redis thÃ nh cÃ´ng')
        except Exception as e : 
            print(f'Káº¿t ná»‘i Redis tháº¥t báº¡i {e}')
            raise e
        print(f'Cache loading model {EMBEDDING_MODEL_NAME}')
        self.model =embedding_cache.get_model('AITeamVN/Vietnamese_Embedding')
        real_dim = self.model.get_sentence_embedding_dimension()
        if real_dim != VECTOR_DIM : 
            print(f"âš ï¸ [WARNING] Config lÃ  {VECTOR_DIM} nhÆ°ng model lÃ  {real_dim}. Äang tá»± Ä‘á»™ng fix...")
            self.vector_dim = real_dim
        else : 
            self.vector_dim = VECTOR_DIM
        print ('Load cache model thÃ nh cÃ´ng')

        self._create_index()

    def _create_index (self) : 
        try : 
            info = self.r.ft(INDEX_NAME).info()
            print(f'Redis index {INDEX_NAME} Ä‘Ã£ tá»“n táº¡i')
        except  : 
            print('Äang táº¡o má»™t index má»›i ')
            schema = (
                TextField("original_query"),
                TextField("response_json"),
                VectorField("embedding",
                        "FLAT", {
                            "TYPE" : "FLOAT32",
                            "DIM" : self.vector_dim,
                            "DISTANCE_METRIC" : "COSINE"
                        }
                            )
            )
            definition = IndexDefinition(prefix=["intent:"], index_type=IndexType.HASH)
            self.r.ft(INDEX_NAME).create_index(schema, definition=definition)
    
    def get_embedding(self, text : str) -> bytes : 
        vector = self.model.encode(text)
        vector_np = np.array(vector)
        return vector_np.astype(np.float32).tobytes()
    
    def check_cache(self, user_query : str, thresold : float = 0.15) -> Optional[Dict] : 
        try : 
            query_vector = self.get_embedding(user_query)
            q = Query(f"*=>[KNN 1 @embedding $vec AS score]")\
                .return_fields("response_json", "score", "original_query")\
                .sort_by("score")\
                .dialect(2)
            params = {"vec" : query_vector}
            results = self.r.ft(INDEX_NAME).search(q, query_params=params)
            if results.docs : 
                doc = results.docs[0]
                score = float(doc.score)
                print(f"   ðŸ”Ž [Cache Check] '{user_query}' ~= '{doc.original_query}' (Score: {score:.4f})")
                if score < thresold : 
                    return json.loads(doc.response_json)
            return None
        except Exception as e : 
            print(f'Cache ERROR {e}')
            return None
    def set_cache(self, user_query : str, response_data : Dict, ttl : int = 3600) : 
        try : 
            vector = self.get_embedding(user_query)
            query_hash = hashlib.md5(user_query.encode('utf-8')).hexdigest()
            key = f"intent:{query_hash}"
            pipe = self.r.pipeline()
            pipe.hset(key, mapping={
            "original_query": user_query,
            "response_json": json.dumps(response_data, ensure_ascii=False),
            "embedding": vector
            })
            pipe.expire(key, ttl)
            pipe.execute()

            print(f"âœ… [Cache Saved] ÄÃ£ lÆ°u cache cho: '{user_query}' (TTL: {ttl}s)")
        except Exception as e:
            print(f"âš ï¸ [Cache Save Error] {e}")

class IntentService : 
    def __init__(self, model : str =None) : 
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self.cache = SemanticCache()
        self.generation_config = {
               # "temperature": 0.1,        # bá»›t sÃ¡ng táº¡o
               # "top_p": 0.8,
               # "top_k": 20,
                "max_output_tokens": 512,
              #  "response_mime_type": "application/json", 
        }
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE, 
}
        if(model is None) : 
            
            self.model = genai.GenerativeModel(model_name='gemma-3-27b-it', 
                                               generation_config=self.generation_config,
                                                safety_settings= self.safety_settings)
        else : 
            self.model = genai.GenerativeModel(model_name=model, 
                                               generation_config=self.generation_config,
                                               safety_settings= self.safety_settings)
    async def detectIntent(self, user_text : str) -> dict :
        try : 
            prompt = f"""
            Báº¡n lÃ  chuyÃªn gia phÃ¢n loáº¡i Ã½ Ä‘á»‹nh (NLU) cho há»‡ thá»‘ng bÃ¡n tÃ i khoáº£n sá»‘ VTV_KEY.
            
            NHIá»†M Vá»¤ Cá»¦A Báº N:
            PhÃ¢n tÃ­ch cÃ¢u nÃ³i cá»§a ngÆ°á»i dÃ¹ng vÃ  tráº£ vá» JSON duy nháº¥t.

            QUY TRÃŒNH SUY LUáº¬N (LÃ m ngáº§m, khÃ´ng in ra):
            1. XÃ¡c Ä‘á»‹nh Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c nháº¯c Ä‘áº¿n: LÃ  "Bot/AI" (danh tÃ­nh) hay lÃ  "Sáº£n pháº©m" (hÃ ng hÃ³a).
            2. Náº¿u lÃ  hÃ ng hÃ³a: NgÆ°á»i dÃ¹ng Ä‘Ã£ gá»i ÄÃšNG TÃŠN (Netflix, ChatGPT) hay chá»‰ gá»i DANH Má»¤C (App xem phim, AI)?
            3. Chá»n Intent phÃ¹ há»£p nháº¥t tá»« danh sÃ¡ch dÆ°á»›i Ä‘Ã¢y.

            DANH SÃCH INTENT :
            
            1. SEARCH_PRODUCT:
               - Khi ngÆ°á»i dÃ¹ng muá»‘n MUA, Há»ŽI GIÃ, Há»ŽI THÃ”NG TIN cá»§a má»™t Sáº¢N PHáº¨M Cá»¤ THá»‚.
               - Dáº¥u hiá»‡u: CÃ³ tÃªn sáº£n pháº©m rÃµ rÃ ng (Netflix, Youtube, Spotify, ChatGPT, Key Win...).
               - VD: "GiÃ¡ Netflix", "Mua acc ChatGPT", "Youtube Premium bao tiá»n".

            2. RECOMMENDATION:
               - Há»i tÆ° váº¥n chung ("NÃªn mua gÃ¬", "Shop cÃ³ gÃ¬").
               - Há»i theo DANH Má»¤C ("CÃ³ pháº§n má»m AI khÃ´ng").
               - Há»i SO SÃNH GIÃ / TÃNH CHáº¤T ("CÃ¡i nÃ o Ä‘áº¯t nháº¥t", "CÃ¡i nÃ o ráº» nháº¥t", "Sáº£n pháº©m nÃ o giÃ¡ cao nháº¥t").
               -...v.v

            3. GET_BEST_SELLER:
               - Chá»‰ dÃ¹ng khi user há»i vá» DOANH Sá», Äá»˜ PHá»” BIáº¾N, HOT TREND,....
               - Keywords: "bÃ¡n cháº¡y nháº¥t", "nhiá»u ngÆ°á»i mua", "hot nháº¥t", "phá»• biáº¿n".
               - LÆ¯U Ã: "Äáº¯t nháº¥t" hay "Ráº» nháº¥t" KHÃ”NG PHáº¢I lÃ  Best Seller.

            4. SUPPORT_RAG:
               - Há»i quy trÃ¬nh: CÃ¡ch thanh toÃ¡n, báº£o hÃ nh, hoÃ n tiá»n, lá»—i Ä‘Äƒng nháº­p, thÃ´ng tin vá» web,....
               - VD: "Thanh toÃ¡n qua momo Ä‘Æ°á»£c khÃ´ng", "LÃ m sao Ä‘á»ƒ nháº­p key".

            5. ORDER_TRACKING:
               - Há»i tráº¡ng thÃ¡i Ä‘Æ¡n hÃ ng (thÆ°á»ng kÃ¨m mÃ£ Ä‘Æ¡n).
               - VD: "ÄÆ¡n #123 xong chÆ°a".

            6. GREETING:
               - ChÃ o há»i xÃ£ giao.
               - Há»i vá» DANH TÃNH Cá»¦A BOT.
               - VD: "Hi shop", "Báº¡n lÃ  ai", "Báº¡n cÃ³ pháº£i ChatGPT khÃ´ng", "Ai táº¡o ra báº¡n".

            LUáº¬T Äáº¶C BIá»†T (TrÃ¡nh Hallucination):
            - Náº¿u user há»i "Báº¡n lÃ  ChatGPT Ã ?" -> GREETING (Há»i danh tÃ­nh).
            - Náº¿u user há»i "BÃ¡n acc ChatGPT khÃ´ng?" -> SEARCH_PRODUCT (Há»i mua hÃ ng).
            - Náº¿u user há»i "Sáº£n pháº©m nÃ y, web nÃ y,.. do ai viáº¿t ra" -> SUPPORT_RAG (Há»i vá» thÃ´ng tin cá»§a web)
            - Náº¿u user nháº¯c "Google" (VD: Acc Google Drive) -> Tráº£ vá» product: "google drive". KHÃ”NG Ä‘Æ°á»£c tá»± sá»­a thÃ nh "youtube".
            - Chuáº©n hÃ³a tÃªn sáº£n pháº©m: "nÃ©t flix" -> "netflix", "ytb" -> "youtube", "chat gpt" -> "chatgpt".

            INPUT NGÆ¯á»œI DÃ™NG: "{user_text}"

            OUTPUT JSON FORMAT (Báº¯t buá»™c):
            {{
                "intent": "TÃŠN_INTENT_VIáº¾T_HOA",
                "entities": {{
                    "product": "tÃªn_sáº£n_pháº©m_chuáº©n_hÃ³a_hoáº·c_null",
                    "order_id": "mÃ£_Ä‘Æ¡n_hÃ ng_hoáº·c_null"
                }}
            }}
            """

            defult_result = {
                "intent" : "GREETING",
                "entities" : {
                    "product" : None,
                    "duration" : None,
                    "order_id" : None
                }
            }

            cached = self.cache.check_cache(user_text)
            if cached : 
                print(f'[Cached] {user_text}')
                return defult_result | cached
            
            print(f"ðŸ¢ [CACHE MISS] '{user_text}' -> Calling AI...")

            respone = await self.model.generate_content_async(prompt)
            raw_text = respone.text.strip()
            clean_text = raw_text.replace("```json", "").replace("```", "").strip()
            result = json.loads(clean_text)
            
            final_result = defult_result | result
            if final_result.get("intent") : 
                self.cache.set_cache(user_text, final_result)
 
            return final_result 
        except Exception as e : 
            print(f"CÃ³ lá»—i xáº£y ra {e} | Raw : {raw_text if 'raw_text' in locals() else None} ")
            return { 
                "intent" : None,
                "entities" : {}
            }
    
#intent_service = IntentService()
if __name__ == "__main__" : 
    import asyncio
    async def main() : 
        #intent_service = IntentService()
        prompt = "NÃªn dÃ¹ng gÃ³i nÃ o giÃ¡ ráº» nhá»‰ ?"
       # result =  await intent_service.detectIntent(prompt)
        #print(result)
    asyncio.run(main())
    